{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import policy_gradient_reinforce as policy_gradient\n",
    "import tensorflow.compat.v1 as tf\n",
    "# Morvan's code is against TF 1.0\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "# Each policy can only increase, decrease, or stay by a certain ACTION_DELTA.\n",
    "NUM_ACTION = 3\n",
    "ACTION_DELTA = 0.01\n",
    "# How many timesteps to look back for state. Ke li 2017 used 25.\n",
    "# TODO: Maybe this is too big and slow.\n",
    "NUM_TIMESTEPS_FOR_STATE = 3\n",
    "# TODO: Tweak episode length and num episodes\n",
    "# The number of episodes / full game. Each episode starts from the initial state w0.\n",
    "NUM_EPISODES = 10\n",
    "# Number of steps per episode\n",
    "EPISODE_LEN = 10\n",
    "\n",
    "def optimize_with_rl(f, w0):\n",
    "  \"\"\"\n",
    "  Params\n",
    "    f - The objective function to be minimized. f(w) should return a scalar.\n",
    "    w0 - Initial w.\n",
    "  Returns\n",
    "    w such that f(w) is minimized\n",
    "  \"\"\"\n",
    "  tf.reset_default_graph()\n",
    "  opt_param_dim = len(w0)\n",
    "  \n",
    "  # Number of dimensions in each state x_t.\n",
    "  # For each timestep to look back, we store the full w(t) vector, and f(w(t))\n",
    "  state_dim = NUM_TIMESTEPS_FOR_STATE * (opt_param_dim + 1)\n",
    "\n",
    "  # Each policies[i] is an RL model for tweaking the 1 parameter dimension w[i]\n",
    "  policies = []\n",
    "  for i in range(opt_param_dim):\n",
    "    RL = policy_gradient.PolicyGradient(\n",
    "      name_suffix=str(i),\n",
    "      n_actions=NUM_ACTION,\n",
    "      n_features=state_dim,\n",
    "      learning_rate=0.02,\n",
    "      reward_decay=1.0\n",
    "    )\n",
    "    policies.append(RL)\n",
    "    print(\"Created policy\" + str(i))\n",
    "\n",
    "  # TODO: Use scipy to get x0. Right now everything is just set to w0, f(w0)\n",
    "  f_w0 = f(w0)\n",
    "  x0 = np.array((w0 + [f_w0]) * NUM_TIMESTEPS_FOR_STATE)\n",
    "  xt = x0\n",
    "  for ep in range(NUM_EPISODES):\n",
    "    for t in range(EPISODE_LEN):\n",
    "      wt = get_most_recent_weights(xt, opt_param_dim)\n",
    "      # Every policy will have its own reward.\n",
    "      rs = np.zeros(opt_param_dim)\n",
    "      for i in range(opt_param_dim):\n",
    "        # The actions are numbers from 0 to NUM_ACTION - 1\n",
    "        action = policies[i].choose_action(xt)\n",
    "        # TODO: Bounding?\n",
    "        wt[i] += convert_action_to_dw(action)\n",
    "        rs[i] = -f(wt)\n",
    "        # TODO: Should we make the state observed by each policy change?\n",
    "        # Right now this means that every time an agent acts,\n",
    "        # the other agents are part of the stochastic environment,\n",
    "        # And yet my reward is only computed off my immediate action.\n",
    "        # Ez change: Just do the xt updates here.\n",
    "        policies[i].store_transition(xt, action, rs[i])\n",
    "      # The end of 1 episode timestep\n",
    "      last_f_wt = -rs[-1]\n",
    "      new_chunk = np.append(wt, last_f_wt) \n",
    "      # Remove the oldest chunk, which is at the front.\n",
    "      xt = np.append(xt[opt_param_dim+1:], new_chunk)\n",
    "    # The end of 1 episode  \n",
    "    for policy in policies:\n",
    "      policy.learn()\n",
    "    print(\"Episode %d, f(w) = %.2f, w = %s\" % (ep+1, last_f_wt, wt))\n",
    "\n",
    "\"\"\"\n",
    "Useful methods for dealing with the state vector that will be used by RL.\n",
    "We have NUM_TIMESTEPS_FOR_STATE chunks, where each chunk has this order\n",
    "  - w(t) vector (There are 'opt_param_dim' of these)\n",
    "  - f(w(t)) - Just one.\n",
    "The chunks are ordered in ascending chronological order.\n",
    "So, the last chunk is the most recent entry\n",
    "\"\"\"\n",
    "def get_most_recent_weights(state, opt_param_dim):\n",
    "  \"\"\"\n",
    "  Get w_t vector, the most recent weights\n",
    "  \"\"\"\n",
    "  return state[-opt_param_dim-1:-1]\n",
    "\n",
    "def convert_action_to_dw(action):\n",
    "  if action == 0:\n",
    "    return 0\n",
    "  elif action == 1:\n",
    "    return -0.01\n",
    "  else:\n",
    "    return 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created policy0\n",
      "Created policy1\n",
      "Episode 1, f(w) = 1.36, w = [2.07 1.04]\n",
      "Episode 2, f(w) = 1.42, w = [2.11 1.07]\n",
      "Episode 3, f(w) = 1.48, w = [2.15 1.1 ]\n",
      "Episode 4, f(w) = 1.71, w = [2.23 1.06]\n",
      "Episode 5, f(w) = 1.90, w = [2.29 1.01]\n",
      "Episode 6, f(w) = 1.99, w = [2.31 0.98]\n",
      "Episode 7, f(w) = 2.06, w = [2.33 0.96]\n",
      "Episode 8, f(w) = 2.25, w = [2.4  0.96]\n",
      "Episode 9, f(w) = 2.37, w = [2.44 0.96]\n",
      "Episode 10, f(w) = 2.42, w = [2.45 0.94]\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "  x = w[0]\n",
    "  y = w[1]\n",
    "  return (x-1)**2 + (y-1.5)**2\n",
    "optimize_with_rl(f, [2,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
