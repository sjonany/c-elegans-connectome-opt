{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import policy_gradient_reinforce as policy_gradient\n",
    "import tensorflow.compat.v1 as tf\n",
    "# Morvan's code is against TF 1.0\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "# Each policy can only increase, decrease, or stay by a certain ACTION_DELTA.\n",
    "NUM_ACTION = 3\n",
    "ACTION_DELTA = 0.01\n",
    "# How many timesteps to look back for state. Ke li 2017 used 25.\n",
    "# TODO: Maybe this is too big and slow.\n",
    "NUM_TIMESTEPS_FOR_STATE = 3\n",
    "# TODO: Tweak episode length and num episodes\n",
    "# The number of episodes / full game. Each episode starts from the initial state w0.\n",
    "NUM_EPISODES = 100\n",
    "# Number of steps per episode\n",
    "EPISODE_LEN = 100\n",
    "\n",
    "def optimize_with_rl(f, w0):\n",
    "  \"\"\"\n",
    "  Params\n",
    "    f - The objective function to be minimized. f(w) should return a scalar.\n",
    "    w0 - Initial w.\n",
    "  Returns\n",
    "    w such that f(w) is minimized\n",
    "  \"\"\"\n",
    "  tf.reset_default_graph()\n",
    "  opt_param_dim = len(w0)\n",
    "  \n",
    "  # Number of dimensions in each state x_t.\n",
    "  # For each timestep to look back, we store the full w(t) vector, and f(w(t))\n",
    "  state_dim = NUM_TIMESTEPS_FOR_STATE * (opt_param_dim + 1)\n",
    "\n",
    "  # Each policies[i] is an RL model for tweaking the 1 parameter dimension w[i]\n",
    "  policies = []\n",
    "  for i in range(opt_param_dim):\n",
    "    RL = policy_gradient.PolicyGradient(\n",
    "      name_suffix=str(i),\n",
    "      n_actions=NUM_ACTION,\n",
    "      n_features=state_dim,\n",
    "      learning_rate=0.02,\n",
    "      reward_decay=1.0\n",
    "    )\n",
    "    policies.append(RL)\n",
    "    print(\"Created policy\" + str(i))\n",
    "\n",
    "  # TODO: Use scipy to get x0. Right now everything is just set to w0, f(w0)\n",
    "  f_w0 = f(w0)\n",
    "  x0 = np.array((w0 + [f_w0]) * NUM_TIMESTEPS_FOR_STATE)\n",
    "  xt = x0\n",
    "  for ep in range(NUM_EPISODES):\n",
    "    for t in range(EPISODE_LEN):\n",
    "      wt = get_most_recent_weights(xt, opt_param_dim)\n",
    "      # Every policy will have its own reward.\n",
    "      rs = np.zeros(opt_param_dim)\n",
    "      for i in range(opt_param_dim):\n",
    "        # The actions are numbers from 0 to NUM_ACTION - 1\n",
    "        action = policies[i].choose_action(xt)\n",
    "        # TODO: Bounding?\n",
    "        wt[i] += convert_action_to_dw(action)\n",
    "        rs[i] = -f(wt)\n",
    "        # TODO: Should we make the state observed by each policy change?\n",
    "        # Right now this means that every time an agent acts,\n",
    "        # the other agents are part of the stochastic environment,\n",
    "        # And yet my reward is only computed off my immediate action.\n",
    "        # Ez change: Just do the xt updates here.\n",
    "        policies[i].store_transition(xt, action, rs[i])\n",
    "      # The end of 1 episode timestep\n",
    "      last_f_wt = -rs[-1]\n",
    "      new_chunk = np.append(wt, last_f_wt) \n",
    "      # Remove the oldest chunk, which is at the front.\n",
    "      xt = np.append(xt[opt_param_dim+1:], new_chunk)\n",
    "    # The end of 1 episode  \n",
    "    for policy in policies:\n",
    "      policy.learn()\n",
    "    print(\"Episode %d, f(w) = %.2f, w = %s\" % (ep+1, last_f_wt, wt))\n",
    "\n",
    "\"\"\"\n",
    "Useful methods for dealing with the state vector that will be used by RL.\n",
    "We have NUM_TIMESTEPS_FOR_STATE chunks, where each chunk has this order\n",
    "  - w(t) vector (There are 'opt_param_dim' of these)\n",
    "  - f(w(t)) - Just one.\n",
    "The chunks are ordered in ascending chronological order.\n",
    "So, the last chunk is the most recent entry\n",
    "\"\"\"\n",
    "def get_most_recent_weights(state, opt_param_dim):\n",
    "  \"\"\"\n",
    "  Get w_t vector, the most recent weights\n",
    "  \"\"\"\n",
    "  # Need to np.copy because np array slices still points to same memory location.\n",
    "  return np.copy(state[-opt_param_dim-1:-1])\n",
    "\n",
    "def convert_action_to_dw(action):\n",
    "  if action == 0:\n",
    "    return 0\n",
    "  elif action == 1:\n",
    "    return -0.01\n",
    "  else:\n",
    "    return 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created policy0\n",
      "Episode 1, f(w) = 0.79, w = [1.89]\n",
      "Episode 2, f(w) = 0.10, w = [1.31]\n",
      "Episode 3, f(w) = 0.04, w = [0.79]\n",
      "Episode 4, f(w) = 0.28, w = [0.47]\n",
      "Episode 5, f(w) = 0.67, w = [0.18]\n",
      "Episode 6, f(w) = 1.25, w = [-0.12]\n",
      "Episode 7, f(w) = 1.69, w = [-0.3]\n",
      "Episode 8, f(w) = 2.13, w = [-0.46]\n",
      "Episode 9, f(w) = 2.82, w = [-0.68]\n",
      "Episode 10, f(w) = 2.99, w = [-0.73]\n",
      "Episode 11, f(w) = 3.50, w = [-0.87]\n",
      "Episode 12, f(w) = 4.37, w = [-1.09]\n",
      "Episode 13, f(w) = 6.05, w = [-1.46]\n",
      "Episode 14, f(w) = 8.29, w = [-1.88]\n",
      "Episode 15, f(w) = 12.53, w = [-2.54]\n",
      "Episode 16, f(w) = 17.64, w = [-3.2]\n",
      "Episode 17, f(w) = 23.23, w = [-3.82]\n",
      "Episode 18, f(w) = 29.59, w = [-4.44]\n",
      "Episode 19, f(w) = 37.82, w = [-5.15]\n",
      "Episode 20, f(w) = 45.43, w = [-5.74]\n",
      "Episode 21, f(w) = 54.32, w = [-6.37]\n",
      "Episode 22, f(w) = 66.42, w = [-7.15]\n",
      "Episode 23, f(w) = 79.21, w = [-7.9]\n",
      "Episode 24, f(w) = 91.58, w = [-8.57]\n",
      "Episode 25, f(w) = 108.16, w = [-9.4]\n",
      "Episode 26, f(w) = 125.22, w = [-10.19]\n",
      "Episode 27, f(w) = 141.37, w = [-10.89]\n",
      "Episode 28, f(w) = 160.02, w = [-11.65]\n",
      "Episode 29, f(w) = 179.56, w = [-12.4]\n",
      "Episode 30, f(w) = 201.36, w = [-13.19]\n",
      "Episode 31, f(w) = 227.71, w = [-14.09]\n",
      "Episode 32, f(w) = 251.22, w = [-14.85]\n",
      "Episode 33, f(w) = 276.89, w = [-15.64]\n",
      "Episode 34, f(w) = 304.85, w = [-16.46]\n",
      "Episode 35, f(w) = 330.88, w = [-17.19]\n",
      "Episode 36, f(w) = 361.38, w = [-18.01]\n",
      "Episode 37, f(w) = 394.82, w = [-18.87]\n",
      "Episode 38, f(w) = 428.90, w = [-19.71]\n",
      "Episode 39, f(w) = 465.70, w = [-20.58]\n",
      "Episode 40, f(w) = 502.21, w = [-21.41]\n",
      "Episode 41, f(w) = 541.03, w = [-22.26]\n",
      "Episode 42, f(w) = 581.77, w = [-23.12]\n",
      "Episode 43, f(w) = 623.50, w = [-23.97]\n",
      "Episode 44, f(w) = 667.71, w = [-24.84]\n",
      "Episode 45, f(w) = 707.03, w = [-25.59]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2503a7c7278c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf1d_w0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moptimize_with_rl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1d_w0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-44296e336172>\u001b[0m in \u001b[0;36moptimize_with_rl\u001b[0;34m(f, w0)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_param_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# The actions are numbers from 0 to NUM_ACTION - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# TODO: Bounding?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mwt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mconvert_action_to_dw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/code/c-elegans-connectome-opt/analysis/policy_gradient_reinforce.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mprob_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_act_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_obs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# select action w.r.t the actions prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 960\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1168\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \"\"\"\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_attrs_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def f2d(w):\n",
    "  x = w[0]\n",
    "  y = w[1]\n",
    "  return (x-1)**2 + (y-1.5)**2\n",
    "f2d_w0 = [2,1]\n",
    "\n",
    "def f1d(w):\n",
    "  return (w[0]-1)**2\n",
    "f1d_w0 = [2]\n",
    "\n",
    "optimize_with_rl(f1d, f1d_w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
