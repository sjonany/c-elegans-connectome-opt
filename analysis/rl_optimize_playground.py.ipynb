{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import policy_gradient_reinforce as policy_gradient\n",
    "import tensorflow.compat.v1 as tf\n",
    "# Morvan's code is against TF 1.0\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "# Each policy can only increase, decrease, or stay by a certain ACTION_DELTA.\n",
    "NUM_ACTION = 3\n",
    "\n",
    "# TODO: Tweak all the parameters below per optimization problem\n",
    "ACTION_DELTA = 0.1\n",
    "# How many timesteps to look back for state. Ke li 2017 used 25.\n",
    "NUM_TIMESTEPS_FOR_STATE = 25\n",
    "# The number of episodes / full game. Each episode starts from the initial state w0.\n",
    "NUM_EPISODES = 100\n",
    "# Number of steps per episode\n",
    "EPISODE_LEN = 15\n",
    "\n",
    "def optimize_with_rl(f, w0):\n",
    "  \"\"\"\n",
    "  Params\n",
    "    f - The objective function to be minimized. f(w) should return a scalar.\n",
    "    w0 - Initial w.\n",
    "  Returns\n",
    "    w such that f(w) is minimized\n",
    "  \"\"\"\n",
    "  tf.reset_default_graph()\n",
    "  opt_param_dim = len(w0)\n",
    "  \n",
    "  # Number of dimensions in each state x_t.\n",
    "  # State space is as described by Ke li 2017:\n",
    "  #    Changes in the objective value at the current location relative to the objective value\n",
    "  #    at the ith most recent location for all i\n",
    "  # We do NOT include the current weights as state like Ke li 2017:\n",
    "  # \"The current location is only used to compute the cost; because the policy\n",
    "  # \"should not depend on the absolute coordinates of the current location,\n",
    "  # \"we exclude it from the input that is fed into the neural net\"\n",
    "  state_dim = NUM_TIMESTEPS_FOR_STATE\n",
    "\n",
    "  # Each policies[i] is an RL model for tweaking the 1 parameter dimension w[i]\n",
    "  policies = []\n",
    "  for i in range(opt_param_dim):\n",
    "    RL = policy_gradient.PolicyGradient(\n",
    "      name_suffix=str(i),\n",
    "      n_actions=NUM_ACTION,\n",
    "      n_features=state_dim,\n",
    "      learning_rate=0.02,\n",
    "      reward_decay=1.0\n",
    "    )\n",
    "    policies.append(RL)\n",
    "\n",
    "  # TODO: Use scipy to get x0. Right now everything is just set to w0, f(w0)\n",
    "  f_w0 = f(w0)\n",
    "  \n",
    "  # wt and f_histories are not explicitly encoded as states, but are necessary to be updated\n",
    "  # because they are used for next state calculations.\n",
    "  wt = w0\n",
    "  # Stores the most recent f(w(t)) histories. First element is the most recent.\n",
    "  # So, we go [f(wt), f(wt-1),...]\n",
    "  f_histories = np.array([f_w0] * NUM_TIMESTEPS_FOR_STATE)\n",
    "  # Initially, we don't have any changes in obj functions\n",
    "  x0 = np.array([0.0] * NUM_TIMESTEPS_FOR_STATE)\n",
    "  xt = x0\n",
    "  for ep in range(NUM_EPISODES):\n",
    "    for t in range(EPISODE_LEN):\n",
    "      # Every policy will have its own reward.\n",
    "      rs = np.zeros(opt_param_dim)\n",
    "      for i in range(opt_param_dim):\n",
    "        # The actions are numbers from 0 to NUM_ACTION - 1\n",
    "        action = policies[i].choose_action(xt)\n",
    "        # TODO: Bounding?\n",
    "        wt[i] += convert_action_to_dw(action)\n",
    "        rs[i] = -f(wt)\n",
    "        # TODO: Should we make the state observed by each policy change?\n",
    "        # Right now this means that every time an agent acts,\n",
    "        # the other agents are part of the stochastic environment,\n",
    "        # And yet my reward is only computed off my immediate action.\n",
    "        # Ez change: Just do the xt updates here.\n",
    "        policies[i].store_transition(xt, action, rs[i])\n",
    "        #print(\"t=%d i=%d a=%d wt=%s rs = %s xt=%s\" %\n",
    "        #      (t, i, action, wt, rs, xt))\n",
    "      # The end of 1 episode timestep\n",
    "      last_f_wt = -rs[-1]\n",
    "      xt = last_f_wt - f_histories\n",
    "      # Rotate f_histories w/ the most recent f_wt entry\n",
    "      f_histories = np.append(last_f_wt, f_histories[:-1])\n",
    "    # The end of 1 episode  \n",
    "    for policy in policies:\n",
    "      policy.learn()\n",
    "    print(\"Episode %d, f(w) = %.2f, w = %s\" % (ep+1, last_f_wt, wt))\n",
    "\n",
    "def convert_action_to_dw(action):\n",
    "  if action == 0:\n",
    "    return 0\n",
    "  elif action == 1:\n",
    "    return -ACTION_DELTA\n",
    "  else:\n",
    "    return ACTION_DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, f(w) = 1.21, w = [2.1]\n",
      "Episode 2, f(w) = 1.21, w = [2.1]\n",
      "Episode 3, f(w) = 1.00, w = [2.0]\n",
      "Episode 4, f(w) = 0.64, w = [1.7999999999999998]\n",
      "Episode 5, f(w) = 0.64, w = [1.7999999999999998]\n",
      "Episode 6, f(w) = 0.25, w = [1.4999999999999996]\n",
      "Episode 7, f(w) = 0.49, w = [1.6999999999999997]\n",
      "Episode 8, f(w) = 0.64, w = [1.7999999999999998]\n",
      "Episode 9, f(w) = 0.25, w = [1.4999999999999996]\n",
      "Episode 10, f(w) = 0.49, w = [1.6999999999999997]\n",
      "Episode 11, f(w) = 0.09, w = [1.2999999999999994]\n",
      "Episode 12, f(w) = 0.04, w = [1.1999999999999993]\n",
      "Episode 13, f(w) = 0.04, w = [1.1999999999999993]\n",
      "Episode 14, f(w) = 0.64, w = [1.7999999999999998]\n",
      "Episode 15, f(w) = 1.00, w = [2.0]\n",
      "Episode 16, f(w) = 0.64, w = [1.7999999999999998]\n",
      "Episode 17, f(w) = 1.96, w = [2.4000000000000004]\n",
      "Episode 18, f(w) = 3.61, w = [2.900000000000001]\n",
      "Episode 19, f(w) = 6.25, w = [3.5000000000000013]\n",
      "Episode 20, f(w) = 7.29, w = [3.7000000000000015]\n",
      "Episode 21, f(w) = 11.56, w = [4.4]\n",
      "Episode 22, f(w) = 16.00, w = [4.999999999999998]\n",
      "Episode 23, f(w) = 26.01, w = [6.099999999999994]\n",
      "Episode 24, f(w) = 33.64, w = [6.799999999999992]\n",
      "Episode 25, f(w) = 42.25, w = [7.499999999999989]\n",
      "Episode 26, f(w) = 57.76, w = [8.599999999999985]\n",
      "Episode 27, f(w) = 79.21, w = [9.89999999999998]\n",
      "Episode 28, f(w) = 108.16, w = [11.399999999999975]\n",
      "Episode 29, f(w) = 127.69, w = [12.299999999999972]\n",
      "Episode 30, f(w) = 151.29, w = [13.299999999999969]\n",
      "Episode 31, f(w) = 187.69, w = [14.699999999999964]\n",
      "Episode 32, f(w) = 231.04, w = [16.19999999999996]\n",
      "Episode 33, f(w) = 272.25, w = [17.49999999999998]\n",
      "Episode 34, f(w) = 306.25, w = [18.499999999999993]\n",
      "Episode 35, f(w) = 361.00, w = [20.000000000000014]\n",
      "Episode 36, f(w) = 420.25, w = [21.500000000000036]\n",
      "Episode 37, f(w) = 479.61, w = [22.900000000000055]\n",
      "Episode 38, f(w) = 542.89, w = [24.300000000000075]\n",
      "Episode 39, f(w) = 600.25, w = [25.500000000000092]\n",
      "Episode 40, f(w) = 676.00, w = [27.000000000000114]\n",
      "Episode 41, f(w) = 756.25, w = [28.500000000000135]\n",
      "Episode 42, f(w) = 835.21, w = [29.900000000000155]\n",
      "Episode 43, f(w) = 918.09, w = [31.300000000000175]\n",
      "Episode 44, f(w) = 1004.89, w = [32.700000000000195]\n",
      "Episode 45, f(w) = 1095.61, w = [34.100000000000215]\n",
      "Episode 46, f(w) = 1197.16, w = [35.600000000000236]\n",
      "Episode 47, f(w) = 1303.21, w = [37.10000000000026]\n",
      "Episode 48, f(w) = 1398.76, w = [38.400000000000276]\n",
      "Episode 49, f(w) = 1513.21, w = [39.9000000000003]\n",
      "Episode 50, f(w) = 1624.09, w = [41.30000000000032]\n",
      "Episode 51, f(w) = 1747.24, w = [42.80000000000034]\n",
      "Episode 52, f(w) = 1874.89, w = [44.30000000000036]\n",
      "Episode 53, f(w) = 2007.04, w = [45.80000000000038]\n",
      "Episode 54, f(w) = 2134.44, w = [47.2000000000004]\n",
      "Episode 55, f(w) = 2265.76, w = [48.60000000000042]\n",
      "Episode 56, f(w) = 2410.81, w = [50.10000000000044]\n",
      "Episode 57, f(w) = 2560.36, w = [51.60000000000046]\n",
      "Episode 58, f(w) = 2704.00, w = [53.00000000000048]\n",
      "Episode 59, f(w) = 2862.25, w = [54.500000000000504]\n",
      "Episode 60, f(w) = 3014.01, w = [55.900000000000524]\n",
      "Episode 61, f(w) = 3180.96, w = [57.400000000000546]\n",
      "Episode 62, f(w) = 3352.41, w = [58.90000000000057]\n",
      "Episode 63, f(w) = 3504.64, w = [60.200000000000585]\n",
      "Episode 64, f(w) = 3684.49, w = [61.70000000000061]\n",
      "Episode 65, f(w) = 3831.61, w = [62.900000000000624]\n",
      "Episode 66, f(w) = 4006.89, w = [64.30000000000062]\n",
      "Episode 67, f(w) = 4199.04, w = [65.80000000000054]\n",
      "Episode 68, f(w) = 4382.44, w = [67.20000000000046]\n",
      "Episode 69, f(w) = 4556.25, w = [68.50000000000038]\n",
      "Episode 70, f(w) = 4733.44, w = [69.80000000000031]\n",
      "Episode 71, f(w) = 4942.09, w = [71.30000000000022]\n",
      "Episode 72, f(w) = 5140.89, w = [72.70000000000014]\n",
      "Episode 73, f(w) = 5329.00, w = [74.00000000000007]\n",
      "Episode 74, f(w) = 5505.64, w = [75.2]\n",
      "Episode 75, f(w) = 5730.49, w = [76.69999999999992]\n",
      "Episode 76, f(w) = 5929.00, w = [77.99999999999984]\n",
      "Episode 77, f(w) = 6146.56, w = [79.39999999999976]\n",
      "Episode 78, f(w) = 6368.04, w = [80.79999999999968]\n",
      "Episode 79, f(w) = 6609.69, w = [82.2999999999996]\n",
      "Episode 80, f(w) = 6806.25, w = [83.49999999999953]\n",
      "Episode 81, f(w) = 7039.21, w = [84.89999999999945]\n",
      "Episode 82, f(w) = 7242.01, w = [86.09999999999938]\n",
      "Episode 83, f(w) = 7464.96, w = [87.39999999999931]\n",
      "Episode 84, f(w) = 7691.29, w = [88.69999999999924]\n",
      "Episode 85, f(w) = 7938.81, w = [90.09999999999916]\n",
      "Episode 86, f(w) = 8118.01, w = [91.0999999999991]\n",
      "Episode 87, f(w) = 8372.25, w = [92.49999999999902]\n",
      "Episode 88, f(w) = 8649.00, w = [93.99999999999893]\n",
      "Episode 89, f(w) = 8854.81, w = [95.09999999999887]\n",
      "Episode 90, f(w) = 9120.25, w = [96.49999999999879]\n",
      "Episode 91, f(w) = 9370.24, w = [97.79999999999872]\n",
      "Episode 92, f(w) = 9623.61, w = [99.09999999999864]\n",
      "Episode 93, f(w) = 9820.81, w = [100.09999999999859]\n",
      "Episode 94, f(w) = 10080.16, w = [101.39999999999851]\n",
      "Episode 95, f(w) = 10363.24, w = [102.79999999999843]\n",
      "Episode 96, f(w) = 10670.89, w = [104.29999999999835]\n",
      "Episode 97, f(w) = 10941.16, w = [105.59999999999827]\n",
      "Episode 98, f(w) = 11257.21, w = [107.09999999999819]\n",
      "Episode 99, f(w) = 11513.29, w = [108.29999999999812]\n",
      "Episode 100, f(w) = 11815.69, w = [109.69999999999804]\n"
     ]
    }
   ],
   "source": [
    "def f2d(w):\n",
    "  x = w[0]\n",
    "  y = w[1]\n",
    "  return (x-1)**2 + (y-1.5)**2\n",
    "f2d_w0 = [2,1]\n",
    "\n",
    "def f1d(w):\n",
    "  return (w[0]-1)**2\n",
    "f1d_w0 = [2]\n",
    "\n",
    "optimize_with_rl(f1d, f1d_w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
